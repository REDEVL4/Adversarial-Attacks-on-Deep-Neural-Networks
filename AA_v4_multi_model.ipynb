{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOaVeHXszMbhWfAgxCoUsBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/REDEVL4/Adversarial-Attacks-on-Deep-Neural-Networks/blob/AA_v4/AA_v4_multi_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szxJEdChQCXr",
        "outputId": "adce8fe7-f0f1-4ec5-b7ee-68202f3658c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.5.1\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.20.1\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio==2.5.1\n",
            "  Downloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1)\n",
            "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
            "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================================================\n",
        "# MURA v1.1 — Multi-backbone training (train/valid only) + robust resume + FGSM/PGD/CW\n",
        "# - LOCAL or Google-Drive-ZIP mode (choose in CONFIG)\n",
        "# - Five models: resnet50, densenet121, googlenet, efficientnet_b0, convnext_tiny\n",
        "# - Per-model hyperparams (LR/WD/dropout/batch) + per-model checkpoint folders\n",
        "# - No leakage: train uses /train, val uses /valid\n",
        "# - Attacks: FGSM, PGD, C&W (torchattacks) with safe defaults + visuals & confusion matrices\n",
        "# =======================================================================================\n",
        "\n",
        "# ----------------\n",
        "# Core imports\n",
        "# ----------------\n",
        "import os, re, time, random, json, math, warnings, zipfile, shutil\n",
        "import tqdm\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score, f1_score, balanced_accuracy_score,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# ================================================================\n",
        "# 0) CONFIG — choose data mode (LOCAL or COLAB_DRIVE_ZIP)\n",
        "# ================================================================\n",
        "MODE = \"COLAB_DRIVE_ZIP\"   # \"LOCAL\" or \"COLAB_DRIVE_ZIP\"\n",
        "\n",
        "# -- LOCAL: point to folder that directly contains 'train' and 'valid'\n",
        "LOCAL_DATA_ROOT = r\"C:\\Users\\reddy\\Downloads\\Documents\\Capstone\\Datasets\\MURA-v1.1\\MURA-v1.1\"\n",
        "\n",
        "# -- COLAB_DRIVE_ZIP: mount & extract once\n",
        "# (safe to leave even if running locally; only used when MODE == \"COLAB_DRIVE_ZIP\")\n",
        "COLAB_ZIP_PATH = \"/content/drive/MyDrive/Capstone - adversarial attack on DNN/Implementation/MURA-v1.1.zip\"\n",
        "COLAB_EXTRACT_DIR = \"/content/mura_data\"\n",
        "COLAB_RUN_DIR     = \"/content/drive/MyDrive/mura_runs_v4\"\n",
        "\n",
        "# Experiment/global defaults (can be overridden per-model via get_model_config)\n",
        "GLOBAL_EPOCHS         = 50           # set larger for full runs\n",
        "GLOBAL_NUM_CLASSES    = 2\n",
        "GLOBAL_SEED           = 42\n",
        "GLOBAL_NUM_WORKERS    = 0           # Windows/CUDA/DirectML -> keep 0 to avoid spawn/pickle issues\n",
        "GLOBAL_PIN_MEMORY     = False\n",
        "\n",
        "# Attacks (tune for runtime)\n",
        "ENABLE_ATTACKS        = True\n",
        "FGSM_EPS              = 2/255\n",
        "PGD_EPS               = 4/255\n",
        "PGD_STEP_ALPHA        = 1/255\n",
        "PGD_STEPS             = 5           # increase if you can afford the time\n",
        "CW_CONFIDENCE         = 0.0         # typical default\n",
        "CW_STEPS              = 100         # fewer steps to keep runtime reasonable\n",
        "CW_LR                 = 0.01\n",
        "\n",
        "# Visualization control (to keep notebooks responsive)\n",
        "VISUALIZE_REALTIME      = True      # draw a few examples during attacks\n",
        "VIS_EVERY_N_BATCHES     = 40        # how often (in batches) to visualize a panel\n",
        "MAX_VIS_IMAGES_PER_STEP = 4         # per visualization step\n",
        "SAVE_FIGS               = True\n",
        "\n",
        "# Models to run\n",
        "MODEL_LIST = [\"googlenet\", \"efficientnet_b0\",\"convnext_tiny\"] #\"resnet50\", \"densenet121\""
      ],
      "metadata": {
        "id": "vks4pYfl_Sao"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 1) Device selection: DirectML -> CUDA -> CPU\n",
        "# ================================================================\n",
        "def pick_device(prefer_gpu=True):\n",
        "    if prefer_gpu:\n",
        "        # 1) DirectML (AMD/Intel on Windows)\n",
        "        try:\n",
        "            import torch_directml\n",
        "            dml_dev = torch_directml.device()\n",
        "            _ = torch.randn(1, device=dml_dev)\n",
        "            print(\"Using DirectML GPU:\", dml_dev)\n",
        "            return dml_dev\n",
        "        except Exception:\n",
        "            pass\n",
        "        # 2) CUDA\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"Using CUDA GPU:\", torch.cuda.get_device_name(0))\n",
        "            return torch.device(\"cuda\")\n",
        "    print(\"Using CPU\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "DEVICE = pick_device(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrgKlwT8_UYF",
        "outputId": "0cdf98c7-46cd-40fe-9579-d0b782fe6f2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 2) Data root setup (LOCAL vs COLAB_DRIVE_ZIP)\n",
        "# ================================================================\n",
        "def _find_mura_root(root: Path) -> Path:\n",
        "    \"\"\"Find the folder that directly contains 'train' and 'valid'.\"\"\"\n",
        "    root = Path(root)\n",
        "    if (root/\"train\").exists() and (root/\"valid\").exists():\n",
        "        return root\n",
        "    for p in root.iterdir():\n",
        "        if p.is_dir() and (p/\"train\").exists() and (p/\"valid\").exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(\"Could not locate 'train/' and 'valid/' under \" + str(root))\n",
        "\n",
        "if MODE.upper() == \"COLAB_DRIVE_ZIP\":\n",
        "    # Mount drive only in Colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "    EXTRACT_DIR = Path(COLAB_EXTRACT_DIR)\n",
        "    EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    if not any(EXTRACT_DIR.iterdir()):\n",
        "        zipp = Path(COLAB_ZIP_PATH)\n",
        "        assert zipp.exists(), f\"ZIP not found: {zipp}\"\n",
        "        print(f\"Extracting {zipp} -> {EXTRACT_DIR} …\")\n",
        "        with zipfile.ZipFile(zipp, 'r') as zf:\n",
        "            zf.extractall(EXTRACT_DIR)\n",
        "        print(\"Extraction complete.\")\n",
        "\n",
        "    DATA_ROOT = str(_find_mura_root(EXTRACT_DIR))\n",
        "    RUN_ROOT  = Path(COLAB_RUN_DIR)\n",
        "    RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "else:\n",
        "    DATA_ROOT = str(_find_mura_root(Path(LOCAL_DATA_ROOT)))\n",
        "    RUN_ROOT  = Path(DATA_ROOT) / \"AA_v4_multi_models_dml_runs_full\"\n",
        "    RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"DATA_ROOT =\", DATA_ROOT)\n",
        "print(\"RUN_ROOT  =\", RUN_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrlwAYxp_W4c",
        "outputId": "fcfec9de-1e42-4b73-afcc-7e6966420660"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extracting /content/drive/MyDrive/Capstone - adversarial attack on DNN/Implementation/MURA-v1.1.zip -> /content/mura_data …\n",
            "Extraction complete.\n",
            "DATA_ROOT = /content/mura_data/MURA-v1.1\n",
            "RUN_ROOT  = /content/drive/MyDrive/mura_runs_v4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 3) Scan MURA (train/valid only) — NO mixing\n",
        "# ================================================================\n",
        "def scan_mura_split(split_root: Path, anatomy_subset=None):\n",
        "    rows = []\n",
        "    for part_dir in sorted(split_root.iterdir()):\n",
        "        if not part_dir.is_dir(): continue\n",
        "        if anatomy_subset and part_dir.name != anatomy_subset: continue\n",
        "        for pat in part_dir.iterdir():\n",
        "            if not pat.is_dir(): continue\n",
        "            for study in pat.iterdir():\n",
        "                if not study.is_dir(): continue\n",
        "                label = 1 if \"positive\" in study.name.lower() else 0\n",
        "                for f in study.iterdir():\n",
        "                    if f.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
        "                        rows.append({\"path\": str(f), \"label\": label})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "ANATOMY_SUBSET = None  # or \"XR_SHOULDER\"\n",
        "train_root = Path(DATA_ROOT)/\"train\"\n",
        "valid_root = Path(DATA_ROOT)/\"valid\"\n",
        "assert train_root.exists() and valid_root.exists(), \"train/valid missing under DATA_ROOT\"\n",
        "\n",
        "train_df = scan_mura_split(train_root, anatomy_subset=ANATOMY_SUBSET)\n",
        "val_df   = scan_mura_split(valid_root, anatomy_subset=ANATOMY_SUBSET)\n",
        "\n",
        "print(f\"Train images: {len(train_df)}  | Val images: {len(val_df)}\")\n",
        "print(\"Label dist (train):\", Counter(train_df[\"label\"].tolist()))\n",
        "print(\"Label dist (val):  \", Counter(val_df[\"label\"].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg55kol5_Y6T",
        "outputId": "4d2ea677-6c47-47b2-8f96-bd6a4ee2bb62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 36812  | Val images: 3197\n",
            "Label dist (train): Counter({0: 21939, 1: 14873})\n",
            "Label dist (val):   Counter({0: 1667, 1: 1530})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 4) Cleaning helpers + study/patient IDs\n",
        "# ================================================================\n",
        "VALID_EXTS = {\".png\", \".jpg\", \".jpeg\"}\n",
        "\n",
        "def clean_df(df):\n",
        "    def ok(p):\n",
        "        pth = Path(p)\n",
        "        return (\n",
        "            (pth.suffix.lower() in VALID_EXTS) and pth.is_file()\n",
        "            and (not pth.name.startswith(\".\")) and (not pth.name.startswith(\"._\"))\n",
        "        )\n",
        "    before = len(df)\n",
        "    df2 = df[df[\"path\"].map(ok)].copy()\n",
        "    drop = before - len(df2)\n",
        "    if drop:\n",
        "        print(f\"[clean_df] removed {drop} bad rows (hidden/missing/non-image).\")\n",
        "    return df2\n",
        "\n",
        "def extract_patient_id(p):\n",
        "    parts = Path(p).parts\n",
        "    pid = next((q for q in parts if q.lower().startswith(\"patient\")), None)\n",
        "    return pid or \"unknown\"\n",
        "\n",
        "def extract_study_id(p):\n",
        "    path = Path(p)\n",
        "    parts = path.parts\n",
        "    patient = next((q for q in parts if q.lower().startswith(\"patient\")), \"unknown\")\n",
        "    # combine patient/study folder\n",
        "    study = parts[parts.index(patient)+1] if patient in parts and (parts.index(patient)+1)<len(parts) else \"study_unknown\"\n",
        "    return f\"{patient}/{study}\"\n",
        "\n",
        "train_df = clean_df(train_df)\n",
        "val_df   = clean_df(val_df)\n",
        "for _df in (train_df, val_df):\n",
        "    _df[\"patient_id\"] = _df[\"path\"].apply(extract_patient_id)\n",
        "    _df[\"study_id\"]   = _df[\"path\"].apply(extract_study_id)\n",
        "    _df.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Tjrq3u_bMi",
        "outputId": "d0062840-8381-4ee0-8561-639a86e10354"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[clean_df] removed 4 bad rows (hidden/missing/non-image).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## In case if wanna test out using some samples for a quick run or for random sampling\n",
        "# train_df = train_df.sample(frac=0.001, random_state=GLOBAL_SEED).reset_index(drop=True)\n",
        "# val_df   = val_df.sample(frac=0.001, random_state=GLOBAL_SEED).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DsiUjO0Ftjmf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 5) Preprocessing (Bone window → CLAHE → Edge mix → Auto-crop)\n",
        "# ================================================================\n",
        "class BoneWindowTransform:\n",
        "    def __init__(self, lp=3, hp=97): self.lp=lp; self.hp=hp\n",
        "    def __call__(self, img):\n",
        "        f = img.astype(np.float32)\n",
        "        nz = f[f>0]\n",
        "        if nz.size==0: return img\n",
        "        p_low, p_high = np.percentile(nz, self.lp), np.percentile(nz, self.hp)\n",
        "        if p_high <= p_low: return img\n",
        "        w = np.clip(f, p_low, p_high)\n",
        "        w = ((w - p_low)/(p_high-p_low)*255.0).astype(np.uint8)\n",
        "        return w\n",
        "\n",
        "class ClaheTransform:\n",
        "    def __init__(self, clip=2.5, tile=(8,8)):\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
        "    def __call__(self, img): return self.clahe.apply(img.astype(np.uint8))\n",
        "\n",
        "class EdgeTransform:\n",
        "    def __init__(self, alpha=0.5): self.alpha=float(alpha)\n",
        "    def __call__(self, img):\n",
        "        f = img.astype(np.float32)\n",
        "        gx = cv2.Sobel(f, cv2.CV_32F, 1, 0, 3); gy = cv2.Sobel(f, cv2.CV_32F, 0, 1, 3)\n",
        "        mag = np.sqrt(gx*gx + gy*gy); mag = (mag/(mag.max()+1e-6))*255.0\n",
        "        out = np.clip(f + self.alpha*mag, 0, 255).astype(np.uint8)\n",
        "        return out\n",
        "\n",
        "class AutoCropTransform:\n",
        "    def __init__(self, thresh=5): self.thresh=thresh\n",
        "    def __call__(self, img):\n",
        "        _, bw = cv2.threshold(img, self.thresh, 255, cv2.THRESH_BINARY)\n",
        "        cnts,_ = cv2.findContours(bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not cnts: return img\n",
        "        x,y,w,h = cv2.boundingRect(max(cnts, key=cv2.contourArea))\n",
        "        if w<16 or h<16: return img\n",
        "        return img[y:y+h, x:x+w]\n",
        "\n",
        "window   = BoneWindowTransform(3,97)\n",
        "clahe    = ClaheTransform(2.5,(8,8))\n",
        "edge     = EdgeTransform(0.5)\n",
        "autocrop = AutoCropTransform(5)\n",
        "\n",
        "def preprocess_chain(img):\n",
        "    x = window(img); x = clahe(x); x = edge(x); x = autocrop(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "sgXM9JTj_dyF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Transformation\n",
        "IMAGENET_MEAN = [0.485,0.456,0.406]\n",
        "IMAGENET_STD  = [0.229,0.224,0.225]\n",
        "\n",
        "def make_transforms(img_size):\n",
        "    tf_eval = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    ])\n",
        "    tf_train = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomResizedCrop(img_size, scale=(0.9, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    ])\n",
        "    return tf_train, tf_eval\n",
        "\n",
        "def safe_imread(p):\n",
        "    img = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {p}\")\n",
        "    return img\n",
        "\n",
        "# dataset returns (tensor, label, study_id) to support study-level eval\n",
        "class MuraDataset(Dataset):\n",
        "    def __init__(self, df, img_size=320, augment=False):\n",
        "        self.paths  = df[\"path\"].astype(str).tolist()\n",
        "        self.labels = df[\"label\"].astype(int).tolist()\n",
        "        self.studies= df[\"study_id\"].astype(str).tolist()\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.tf_train, self.tf_eval = make_transforms(img_size)\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]; y = self.labels[idx]\n",
        "        img = preprocess_chain(safe_imread(p))\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_AREA)\n",
        "        img3 = np.stack([img,img,img], axis=-1)\n",
        "        tfm = self.tf_train if self.augment else self.tf_eval\n",
        "        x = tfm(img3)\n",
        "        return x, y, self.studies[idx]\n"
      ],
      "metadata": {
        "id": "X94XYDdt_hQL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 6) Models + per-model hyperparams\n",
        "# ================================================================\n",
        "def get_model_config(name: str):\n",
        "    name = name.lower()\n",
        "    common = dict(num_classes=GLOBAL_NUM_CLASSES, label_smooth=0.05)\n",
        "    if name == \"resnet50\":\n",
        "        return dict(**common, img_size=320, batch_size=24, lr=3e-4, weight_decay=1e-4, dropout=0.40)\n",
        "    if name == \"densenet121\":\n",
        "        return dict(**common, img_size=320, batch_size=16, lr=2e-4, weight_decay=1e-4, dropout=0.20)\n",
        "    if name == \"googlenet\":\n",
        "        return dict(**common, img_size=320, batch_size=24, lr=3e-4, weight_decay=1e-4, dropout=0.30)\n",
        "    if name == \"efficientnet_b0\":\n",
        "        return dict(**common, img_size=320, batch_size=24, lr=1e-4, weight_decay=1e-5, dropout=0.30)\n",
        "    if name == \"convnext_tiny\":\n",
        "        return dict(**common, img_size=320, batch_size=24, lr=2e-4, weight_decay=5e-5, dropout=0.10)\n",
        "    raise ValueError(f\"Unknown model: {name}\")\n",
        "\n",
        "def build_model(name=\"resnet50\", num_classes=2, dropout_p=0.4):\n",
        "    name = name.lower()\n",
        "    if name == \"resnet50\":\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "        in_f = m.fc.in_features\n",
        "        m.fc = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(in_f, num_classes))\n",
        "    elif name == \"densenet121\":\n",
        "        m = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        in_f = m.classifier.in_features\n",
        "        m.classifier = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(in_f, num_classes))\n",
        "    # elif name == \"googlenet\":\n",
        "    #     m = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1, aux_logits=True)\n",
        "    #     in_f = m.fc.in_features\n",
        "    #     m.fc = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(in_f, num_classes))\n",
        "    elif name == \"googlenet\":\n",
        "        m = models.googlenet(\n",
        "            weights=models.GoogLeNet_Weights.IMAGENET1K_V1,\n",
        "            aux_logits=True\n",
        "        )\n",
        "        # also force off in case someone toggles it later\n",
        "        m.aux_logits = True\n",
        "        in_f = m.fc.in_features\n",
        "        m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_f, num_classes))\n",
        "\n",
        "    elif name == \"efficientnet_b0\":\n",
        "        m = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        in_f = m.classifier[-1].in_features\n",
        "        # keep its internal dropout and add ours before final linear\n",
        "        m.classifier[-1] = nn.Identity()\n",
        "        m.classifier = nn.Sequential(*list(m.classifier), nn.Dropout(dropout_p), nn.Linear(in_f, num_classes))\n",
        "    elif name == \"convnext_tiny\":\n",
        "        m = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "        in_f = m.classifier[-1].in_features\n",
        "        m.classifier[-1] = nn.Identity()\n",
        "        m.classifier = nn.Sequential(*list(m.classifier), nn.Dropout(dropout_p), nn.Linear(in_f, num_classes))\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model: \" + name)\n",
        "    return m\n",
        "\n"
      ],
      "metadata": {
        "id": "JruNWftC_n3p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 7) Checkpoints (per-model directories) + resume helpers\n",
        "# ================================================================\n",
        "def ensure_dir(p: Path):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def ckpt_paths_for_model(run_root: Path, model_name: str):\n",
        "    base = ensure_dir(run_root / \"checkpoints\" / model_name)\n",
        "    return {\n",
        "        \"latest\": base / \"latest.pt\",\n",
        "        \"best\":   run_root / f\"{model_name}_best_by_auc.pt\",\n",
        "        \"history_json\": base / \"history.json\",\n",
        "        \"snapshots\": ensure_dir(base / \"snapshots\")\n",
        "    }\n",
        "\n",
        "def get_rng_state():\n",
        "    return {\n",
        "        \"py_random\": random.getstate(),\n",
        "        \"np_random\": np.random.get_state(),\n",
        "        \"torch_cpu\": torch.get_rng_state().tolist(),\n",
        "        \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "    }\n",
        "\n",
        "def set_rng_state(st):\n",
        "    try: random.setstate(st[\"py_random\"])\n",
        "    except: pass\n",
        "    try: np.random.set_state(st[\"np_random\"])\n",
        "    except: pass\n",
        "    try: torch.set_rng_state(torch.tensor(st[\"torch_cpu\"], dtype=torch.uint8))\n",
        "    except: pass\n",
        "    if torch.cuda.is_available() and st.get(\"torch_cuda\") is not None:\n",
        "        try: torch.cuda.set_rng_state_all(st[\"torch_cuda\"])\n",
        "        except: pass\n",
        "\n",
        "def atomic_save(state, path: Path):\n",
        "    tmp = str(path) + \".tmp\"\n",
        "    torch.save(state, tmp)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def save_history_json(history_dict, path: Path):\n",
        "    try:\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(history_dict, f, indent=2)\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] could not write {path}: {e}\")\n",
        "\n",
        "def save_checkpoint_for_model(ck, epoch_idx, best_auc, history, model, optimizer=None, scheduler=None):\n",
        "    state = {\n",
        "        \"epoch\": epoch_idx,\n",
        "        \"model_name\": model.__class__.__name__.lower(),\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict() if optimizer else None,\n",
        "        \"scheduler_state\": scheduler.state_dict() if scheduler else None,\n",
        "        \"best_auc\": best_auc,\n",
        "        \"history\": history,\n",
        "        \"rng_state\": get_rng_state(),\n",
        "        \"device_info\": str(DEVICE),\n",
        "    }\n",
        "    atomic_save(state, ck[\"latest\"])\n",
        "    save_history_json(history, ck[\"history_json\"])\n",
        "\n",
        "def try_load_checkpoint_for_model(ck, model, optimizer=None, scheduler=None, source=\"latest\",\n",
        "                                  load_optimizer=True, load_scheduler=True, load_rng=True):\n",
        "    if source == \"latest\":\n",
        "        ckpt_path = ck[\"latest\"]\n",
        "    elif isinstance(source, int):\n",
        "        ckpt_path = ck[\"snapshots\"]/f\"epoch_{source:03d}.pt\"\n",
        "    else:\n",
        "        ckpt_path = Path(source)\n",
        "\n",
        "    if not ckpt_path.exists():\n",
        "        print(f\"[warn] Checkpoint not found for {ckpt_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Found checkpoint for {ckpt_path}, loading …\")\n",
        "    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "    if load_optimizer and optimizer and ckpt.get(\"optimizer_state\") is not None:\n",
        "        try: optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "        except Exception as e: print(\"[warn] optimizer not restored:\", e)\n",
        "    if load_scheduler and scheduler and ckpt.get(\"scheduler_state\") is not None:\n",
        "        try: scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "        except Exception as e: print(\"[warn] scheduler not restored:\", e)\n",
        "    if load_rng and ckpt.get(\"rng_state\") is not None:\n",
        "        set_rng_state(ckpt[\"rng_state\"])\n",
        "\n",
        "    start_epoch = int(ckpt.get(\"epoch\",0)) + 1\n",
        "    best_auc = float(ckpt.get(\"best_auc\",-1.0))\n",
        "    hist = ckpt.get(\"history\", defaultdict(list))\n",
        "    if isinstance(hist, defaultdict): hist=dict(hist)\n",
        "    print(f\"Resume at epoch {start_epoch} (best_auc={best_auc:.4f})\")\n",
        "    return start_epoch, best_auc, hist\n",
        "\n"
      ],
      "metadata": {
        "id": "fEzLD_Ph_rT4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 8) Metrics helpers (image-level + study-level)\n",
        "# ================================================================\n",
        "@torch.no_grad()\n",
        "def predict_probs(model, loader):\n",
        "    model.eval()\n",
        "    all_probs, all_targets, all_studies = [], [], []\n",
        "    for xb, yb, sb in loader:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        logits = model(xb)\n",
        "        if isinstance(logits, (tuple, list)):\n",
        "        # common case: first element is the main logits\n",
        "          logits = logits[0]\n",
        "        elif hasattr(logits, \"logits\"):  # namedtuple with a .logits field\n",
        "          logits = logits.logits\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_probs.append(probs); all_targets.append(yb.numpy()); all_studies.extend(list(sb))\n",
        "    return np.concatenate(all_probs), np.concatenate(all_targets), np.array(all_studies)\n",
        "\n",
        "def metrics_from_probs(probs, targets):\n",
        "    probs = np.asarray(probs)\n",
        "    targets = np.asarray(targets)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    out = {}\n",
        "    try: out[\"AUROC\"] = float(roc_auc_score(targets, probs))\n",
        "    except Exception: out[\"AUROC\"] = float(\"nan\")\n",
        "    out[\"ACC\"]  = float(accuracy_score(targets, preds))\n",
        "    out[\"BACC\"] = float(balanced_accuracy_score(targets, preds))\n",
        "    out[\"F1\"]   = float(f1_score(targets, preds))\n",
        "    return out\n",
        "\n",
        "def study_level_metrics(studies, probs, targets):\n",
        "    df = pd.DataFrame({\"study\": studies, \"prob\": probs, \"y\": targets})\n",
        "    g = df.groupby(\"study\")\n",
        "    probs_s = g[\"prob\"].mean().values\n",
        "    targs_s = g[\"y\"].max().astype(int).values\n",
        "    return metrics_from_probs(probs_s, targs_s)\n",
        "\n",
        "def plot_confusion(y_true, probs, title, out_path=None):\n",
        "    preds = (np.asarray(probs) >= 0.5).astype(int)\n",
        "    cm = confusion_matrix(y_true, preds)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"negative\", \"positive\"])\n",
        "    fig, ax = plt.subplots(figsize=(4,4))\n",
        "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    if out_path: plt.savefig(out_path, dpi=140)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3U5WlpZ5_uRN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 9) Attacks (FGSM/PGD/C&W) — robust wrappers\n",
        "# ================================================================\n",
        "# Try torchattacks; if missing, we implement FGSM/PGD manually\n",
        "USE_TORCHATTACKS = False\n",
        "try:\n",
        "    import torchattacks as ta\n",
        "    USE_TORCHATTACKS = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def fgsm_attack(model, x, y, eps=2/255):\n",
        "    x = x.clone().detach().to(DEVICE); x.requires_grad_(True)\n",
        "    y = y.to(DEVICE)\n",
        "    logits = model(x)\n",
        "    loss = nn.CrossEntropyLoss()(logits, y)\n",
        "    loss.backward()\n",
        "    x_adv = x + eps * x.grad.sign()\n",
        "    return torch.clamp(x_adv, -10, 10).detach()\n",
        "\n",
        "def pgd_attack(model, x, y, eps=4/255, alpha=1/255, iters=7):\n",
        "    x = x.clone().detach().to(DEVICE)\n",
        "    y = y.to(DEVICE)\n",
        "    x_adv = x.clone().detach()\n",
        "    for _ in range(iters):\n",
        "        x_adv.requires_grad_(True)\n",
        "        logits = model(x_adv)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            x_adv = x_adv + alpha * x_adv.grad.sign()\n",
        "            # project back to eps-ball around original x\n",
        "            delta = torch.clamp(x_adv - x, min=-eps, max=eps)\n",
        "            x_adv = torch.clamp(x + delta, -10, 10)\n",
        "    return x_adv.detach()\n",
        "\n",
        "def cw_attack_torchattacks(model, x, y, c=1.0, steps=100, lr=0.01, kappa=0.0):\n",
        "    # Build only once per call with explicit kwargs (no None)\n",
        "    attack = ta.CW(model, c=c, kappa=kappa, steps=steps, lr=lr)\n",
        "    return attack(x, y)\n",
        "\n",
        "def attack_and_eval(model, loader, attack_name, out_dir, params, vis_every=40, max_vis=4):\n",
        "    \"\"\"\n",
        "    Runs attack across loader, logs metrics, and saves occasional visual panels.\n",
        "    attack_name in {\"clean\",\"fgsm\",\"pgd\",\"cw\"}.\n",
        "    params: dict of params for the chosen attack (ignored for \"clean\").\n",
        "    \"\"\"\n",
        "    ensure_dir(out_dir)\n",
        "    model.eval()\n",
        "    all_probs, all_targets = [], []\n",
        "    shown = 0\n",
        "\n",
        "    for b_idx, (xb, yb, sb) in enumerate(loader):\n",
        "        xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
        "\n",
        "        if attack_name == \"clean\":\n",
        "            x_use = xb\n",
        "        elif attack_name == \"fgsm\":\n",
        "            x_use = fgsm_attack(model, xb, yb, eps=float(params.get(\"eps\", FGSM_EPS)))\n",
        "        elif attack_name == \"pgd\":\n",
        "            x_use = pgd_attack(\n",
        "                model, xb, yb,\n",
        "                eps=float(params.get(\"eps\", PGD_EPS)),\n",
        "                alpha=float(params.get(\"alpha\", PGD_STEP_ALPHA)),\n",
        "                iters=int(params.get(\"iters\", PGD_STEPS)),\n",
        "            )\n",
        "        elif attack_name == \"cw\":\n",
        "            if not USE_TORCHATTACKS:\n",
        "                # Fallback: use a strong PGD if CW not available\n",
        "                x_use = pgd_attack(model, xb, yb, eps=PGD_EPS, alpha=PGD_STEP_ALPHA, iters=PGD_STEPS)\n",
        "            else:\n",
        "                x_use = cw_attack_torchattacks(\n",
        "                    model, xb, yb,\n",
        "                    c=float(params.get(\"c\", 1.0)),\n",
        "                    steps=int(params.get(\"steps\", CW_STEPS)),\n",
        "                    lr=float(params.get(\"lr\", CW_LR)),\n",
        "                    kappa=float(params.get(\"kappa\", CW_CONFIDENCE)),\n",
        "                )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown attack: \" + attack_name)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_use)\n",
        "            probs = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "        all_targets.append(yb.detach().cpu().numpy())\n",
        "\n",
        "        # real-time visualization\n",
        "        if VISUALIZE_REALTIME and (b_idx % vis_every == 0):\n",
        "            # show up to max_vis images from this batch\n",
        "            k = min(max_vis, x_use.size(0))\n",
        "            x0 = xb[:k].detach().cpu()\n",
        "            xa = x_use[:k].detach().cpu()\n",
        "            p0 = torch.softmax(model(x0.to(DEVICE)), dim=1)[:,1].detach().cpu().numpy()\n",
        "            pa = torch.softmax(model(xa.to(DEVICE)), dim=1)[:,1].detach().cpu().numpy()\n",
        "            y0 = yb[:k].detach().cpu().numpy()\n",
        "\n",
        "            def denorm(t):\n",
        "                # tensor BCHW -> BHWC in [0,1]\n",
        "                arr = t.numpy().transpose(0,2,3,1)\n",
        "                arr = arr * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
        "                return np.clip(arr, 0, 1)\n",
        "\n",
        "            clean = denorm(x0)\n",
        "            adv   = denorm(xa)\n",
        "            fig, axes = plt.subplots(2, k, figsize=(3.2*k, 6))\n",
        "            for i in range(k):\n",
        "                axes[0,i].imshow(clean[i]); axes[0,i].axis(\"off\")\n",
        "                axes[0,i].set_title(f\"clean y={y0[i]} p1={p0[i]:.2f}\")\n",
        "                axes[1,i].imshow(adv[i]);   axes[1,i].axis(\"off\")\n",
        "                axes[1,i].set_title(f\"{attack_name} p1={pa[i]:.2f}\")\n",
        "            plt.suptitle(f\"{attack_name.upper()} — batch {b_idx}\")\n",
        "            plt.tight_layout()\n",
        "            if SAVE_FIGS:\n",
        "                plt.savefig(out_dir/f\"{attack_name}_vis_batch{b_idx:04d}.png\", dpi=130)\n",
        "            plt.show()\n",
        "            shown += k\n",
        "\n",
        "    probs = np.concatenate(all_probs); targs = np.concatenate(all_targets)\n",
        "    met   = metrics_from_probs(probs, targs)\n",
        "    # confusion matrices\n",
        "    plot_confusion(targs, probs, f\"{attack_name.upper()} — image-level\", out_path=(out_dir/f\"{attack_name}_cm.png\" if SAVE_FIGS else None))\n",
        "    return probs, targs, met\n"
      ],
      "metadata": {
        "id": "qw9HdYJD_5fN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 10) Orchestration: train/resume + validate + attacks (per model)\n",
        "# ================================================================\n",
        "random.seed(GLOBAL_SEED); np.random.seed(GLOBAL_SEED); torch.manual_seed(GLOBAL_SEED)\n",
        "\n",
        "all_runs_summary = []\n",
        "\n",
        "for mname in MODEL_LIST:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Model:\", mname)\n",
        "\n",
        "    # per-model config\n",
        "    cfg = get_model_config(mname)\n",
        "    IMG_SIZE   = cfg[\"img_size\"]\n",
        "    BATCH_SIZE = cfg[\"batch_size\"]\n",
        "    LR         = cfg[\"lr\"]\n",
        "    WD         = cfg[\"weight_decay\"]\n",
        "    DROPOUT    = cfg[\"dropout\"]\n",
        "    LABEL_SMOOTH = cfg[\"label_smooth\"]\n",
        "\n",
        "    # per-model run dirs and checkpoints\n",
        "    model_run_dir = ensure_dir(RUN_ROOT / f\"run_{mname}\")\n",
        "    ck = ckpt_paths_for_model(RUN_ROOT, mname)\n",
        "\n",
        "    # data loaders (train/val only; NO mixing)\n",
        "    train_ds = MuraDataset(train_df, img_size=IMG_SIZE, augment=True)\n",
        "    val_ds   = MuraDataset(val_df,   img_size=IMG_SIZE, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=GLOBAL_NUM_WORKERS, pin_memory=GLOBAL_PIN_MEMORY)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              num_workers=GLOBAL_NUM_WORKERS, pin_memory=GLOBAL_PIN_MEMORY)\n",
        "\n",
        "    # build model + optim\n",
        "    model = build_model(mname, GLOBAL_NUM_CLASSES, dropout_p=DROPOUT).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2) #verbose=True\n",
        "\n",
        "    # resume if possible\n",
        "    history = defaultdict(list)\n",
        "    best_auc = -1.0\n",
        "    start_epoch = 1\n",
        "    resume = try_load_checkpoint_for_model(ck, model, optimizer=optimizer, scheduler=scheduler, source=\"latest\",\n",
        "                                           load_optimizer=True, load_scheduler=True, load_rng=True)\n",
        "    if resume is not None:\n",
        "        start_epoch, best_auc, loaded_hist = resume\n",
        "        for k,v in loaded_hist.items():\n",
        "            history[k] = v if isinstance(v,list) else list(v)\n",
        "        print(f\"Resumed {mname} from epoch {start_epoch}\")\n",
        "\n",
        "    # ------------- TRAIN LOOP -------------\n",
        "    for epoch in range(start_epoch, GLOBAL_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # TRAIN\n",
        "        model.train()\n",
        "        tr_loss_sum = 0.0; tr_seen=0; tr_correct=0\n",
        "        pbar = tqdm.tqdm(train_loader, desc=f\"{mname} Epoch {epoch}/{GLOBAL_EPOCHS}\", leave=False)\n",
        "        for xb, yb, _ in pbar:\n",
        "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(xb)\n",
        "            if isinstance(logits, (tuple, list)):\n",
        "                # common case: first element is the main logits\n",
        "                logits = logits[0]\n",
        "            elif hasattr(logits, \"logits\"):  # namedtuple with a .logits field\n",
        "                logits = logits.logits\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            tr_loss_sum += loss.item()*bs\n",
        "            tr_correct  += (logits.argmax(1)==yb).sum().item()\n",
        "            tr_seen     += bs\n",
        "            pbar.set_postfix({\"loss\": f\"{tr_loss_sum/max(1,tr_seen):.4f}\", \"acc\": f\"{tr_correct/max(1,tr_seen):.4f}\"}, refresh=False)\n",
        "        pbar.close()\n",
        "\n",
        "        # VALID (loss/acc)\n",
        "        model.eval()\n",
        "        va_loss_sum=0.0; va_seen=0; va_correct=0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb, _ in val_loader:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                logits = model(xb)\n",
        "                if isinstance(logits, (tuple, list)):\n",
        "                  # common case: first element is the main logits\n",
        "                  logits = logits[0]\n",
        "                elif hasattr(logits, \"logits\"):  # namedtuple with a .logits field\n",
        "                  logits = logits.logits\n",
        "                loss = criterion(logits, yb)\n",
        "                bs = xb.size(0)\n",
        "                va_loss_sum += loss.item()*bs\n",
        "                va_correct  += (logits.argmax(1)==yb).sum().item()\n",
        "                va_seen     += bs\n",
        "\n",
        "        tr_loss = tr_loss_sum/max(1,tr_seen); tr_acc=tr_correct/max(1,tr_seen)\n",
        "        va_loss = va_loss_sum/max(1,va_seen); va_acc=va_correct/max(1,va_seen)\n",
        "\n",
        "        # VALID AUROC image-level\n",
        "        probs_val, targs_val, studies_val = predict_probs(model, val_loader)\n",
        "        val_img_metrics = metrics_from_probs(probs_val, targs_val)\n",
        "        val_auc = val_img_metrics[\"AUROC\"]\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "        history[\"val_auc\"].append(val_auc)\n",
        "\n",
        "        # Save latest & snapshot\n",
        "        save_checkpoint_for_model(ck, epoch, best_auc, history, model, optimizer, scheduler)\n",
        "        if epoch % 10 == 0:\n",
        "            snap = ck[\"snapshots\"]/f\"epoch_{epoch:03d}.pt\"\n",
        "            torch.save({\n",
        "                \"epoch\": epoch, \"model_name\": mname, \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(), \"scheduler_state\": scheduler.state_dict(),\n",
        "                \"best_auc\": best_auc, \"history\": history, \"rng_state\": get_rng_state()\n",
        "            }, snap)\n",
        "            print(\"Saved snapshot:\", snap)\n",
        "\n",
        "        # Save best-by-AUROC\n",
        "        saved_msg = \"\"\n",
        "        if not np.isnan(val_auc) and val_auc > best_auc + 1e-5:\n",
        "            best_auc = float(val_auc)\n",
        "            torch.save(model.state_dict(), ck[\"best\"])\n",
        "            saved_msg = f\"  saved_best(AUROC={best_auc:.4f})\"\n",
        "\n",
        "        # schedule on val loss\n",
        "        try: scheduler.step(va_loss)\n",
        "        except: pass\n",
        "\n",
        "        print(f\"Epoch {epoch} {mname}: train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f} | val_AUROC={val_auc:.4f}{saved_msg}  time={time.time()-t0:.1f}s\")\n",
        "\n",
        "    # After training: ensure best weights loaded for attacks\n",
        "    if ck[\"best\"].exists():\n",
        "        print(\"Loading best weights for attacks:\", ck[\"best\"])\n",
        "        model.load_state_dict(torch.load(ck[\"best\"], map_location=DEVICE))\n",
        "\n",
        "    # Plot training curves\n",
        "    try:\n",
        "        fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "        ax[0].plot(history[\"train_loss\"], label=\"train_loss\")\n",
        "        ax[0].plot(history[\"val_loss\"],   label=\"val_loss\")\n",
        "        ax[0].legend(); ax[0].set_title(f\"{mname} — Loss\")\n",
        "        ax[1].plot(history[\"train_acc\"], label=\"train_acc\")\n",
        "        ax[1].plot(history[\"val_acc\"],   label=\"val_acc\")\n",
        "        ax[1].plot(history[\"val_auc\"],   label=\"val_AUROC\")\n",
        "        ax[1].legend(); ax[1].set_title(f\"{mname} — Acc/AUROC\")\n",
        "        plt.tight_layout()\n",
        "        if SAVE_FIGS: plt.savefig(model_run_dir/f\"{mname}_curves.png\", dpi=140)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(\"[warn] curve plotting failed:\", e)\n",
        "\n",
        "    # ----------------- Attacks -----------------\n",
        "    attack_summary = {\"model\": mname}\n",
        "\n",
        "    if ENABLE_ATTACKS:\n",
        "        out_adv_dir = ensure_dir(RUN_ROOT / \"adv_examples\" / mname)\n",
        "        # CLEAN\n",
        "        probs_c, targs_c, met_c = attack_and_eval(model, val_loader, \"clean\", out_adv_dir/\"clean\", {},\n",
        "                                                  vis_every=VIS_EVERY_N_BATCHES, max_vis=MAX_VIS_IMAGES_PER_STEP)\n",
        "        attack_summary[\"clean\"] = met_c\n",
        "\n",
        "        # FGSM\n",
        "        probs_f, targs_f, met_f = attack_and_eval(model, val_loader, \"fgsm\", out_adv_dir/\"fgsm\",\n",
        "                                                  {\"eps\": FGSM_EPS},\n",
        "                                                  vis_every=VIS_EVERY_N_BATCHES, max_vis=MAX_VIS_IMAGES_PER_STEP)\n",
        "        attack_summary[\"fgsm\"] = met_f\n",
        "\n",
        "        # PGD\n",
        "        probs_p, targs_p, met_p = attack_and_eval(model, val_loader, \"pgd\", out_adv_dir/\"pgd\",\n",
        "                                                  {\"eps\": PGD_EPS, \"alpha\": PGD_STEP_ALPHA, \"iters\": PGD_STEPS},\n",
        "                                                  vis_every=VIS_EVERY_N_BATCHES, max_vis=MAX_VIS_IMAGES_PER_STEP)\n",
        "        attack_summary[\"pgd\"] = met_p\n",
        "\n",
        "        # C&W (torchattacks if available, else falls back to PGD)\n",
        "        params_cw = {\"c\":1.0, \"steps\":CW_STEPS, \"lr\":CW_LR, \"kappa\":CW_CONFIDENCE}\n",
        "        probs_w, targs_w, met_w = attack_and_eval(model, val_loader, \"cw\", out_adv_dir/\"cw\",\n",
        "                                                  params_cw, vis_every=VIS_EVERY_N_BATCHES, max_vis=MAX_VIS_IMAGES_PER_STEP)\n",
        "        attack_summary[\"cw\"] = met_w\n",
        "\n",
        "        # Overlay comparison bar for this model (ACC/F1/AUROC)\n",
        "        try:\n",
        "            labels = [\"clean\",\"fgsm\",\"pgd\",\"cw\"]\n",
        "            aurocs = [attack_summary[k][\"AUROC\"] for k in labels]\n",
        "            accs   = [attack_summary[k][\"ACC\"]   for k in labels]\n",
        "            f1s    = [attack_summary[k][\"F1\"]    for k in labels]\n",
        "\n",
        "            fig, ax = plt.subplots(1,3, figsize=(13,3.8))\n",
        "            ax[0].bar(labels, aurocs); ax[0].set_title(f\"{mname} AUROC\")\n",
        "            ax[1].bar(labels, accs);   ax[1].set_title(f\"{mname} ACC\")\n",
        "            ax[2].bar(labels, f1s);    ax[2].set_title(f\"{mname} F1\")\n",
        "            for a in ax:\n",
        "                for label in a.get_xticklabels(): label.set_rotation(25)\n",
        "            plt.tight_layout()\n",
        "            if SAVE_FIGS: plt.savefig(model_run_dir/f\"{mname}_attack_bar.png\", dpi=140)\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(\"[warn] attack comparison plotting failed:\", e)\n",
        "\n",
        "    # store per-model summary\n",
        "    all_runs_summary.append({\"model\": mname, \"history\": {k:list(v) for k,v in history.items()}, \"attacks\": attack_summary})\n",
        "\n",
        "# Save the global summary\n",
        "with open(RUN_ROOT/\"results_summary.json\", \"w\") as f:\n",
        "    json.dump(all_runs_summary, f, indent=2)\n",
        "print(\"Saved:\", RUN_ROOT/\"results_summary.json\")\n",
        "\n",
        "# Optional: combined comparison across models (val AUROC last epoch)\n",
        "try:\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    names = []\n",
        "    last_aurocs = []\n",
        "    for rec in all_runs_summary:\n",
        "        names.append(rec[\"model\"])\n",
        "        a = rec[\"history\"].get(\"val_auc\", [])\n",
        "        last_aurocs.append(a[-1] if len(a)>0 else np.nan)\n",
        "    ax.bar(names, last_aurocs)\n",
        "    ax.set_title(\"Validation AUROC (last epoch) per model\")\n",
        "    for lbl in ax.get_xticklabels(): lbl.set_rotation(20)\n",
        "    plt.tight_layout()\n",
        "    if SAVE_FIGS: plt.savefig(RUN_ROOT/\"models_val_auc_bar.png\", dpi=140)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"[warn] summary plotting failed:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82v9gH57_N1P",
        "outputId": "54eda5d6-bb3d-4b2e-8fd5-995f48899f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Model: googlenet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 229MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found checkpoint for /content/drive/MyDrive/mura_runs_v4/checkpoints/googlenet/latest.pt, loading …\n",
            "Resume at epoch 5 (best_auc=0.8565)\n",
            "Resumed googlenet from epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 googlenet: train_loss=0.4450 acc=0.8245 | val_loss=0.4696 acc=0.8104 | val_AUROC=0.8742  saved_best(AUROC=0.8742)  time=630.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 googlenet: train_loss=0.4335 acc=0.8303 | val_loss=0.5017 acc=0.8023 | val_AUROC=0.8781  saved_best(AUROC=0.8781)  time=630.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 googlenet: train_loss=0.4239 acc=0.8381 | val_loss=0.4691 acc=0.8039 | val_AUROC=0.8759  time=626.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 googlenet: train_loss=0.4120 acc=0.8448 | val_loss=0.5150 acc=0.7954 | val_AUROC=0.8631  time=627.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 googlenet: train_loss=0.3998 acc=0.8517 | val_loss=0.5623 acc=0.7998 | val_AUROC=0.8682  time=628.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved snapshot: /content/drive/MyDrive/mura_runs_v4/checkpoints/googlenet/snapshots/epoch_010.pt\n",
            "Epoch 10 googlenet: train_loss=0.3906 acc=0.8570 | val_loss=0.4940 acc=0.7957 | val_AUROC=0.8678  time=634.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 googlenet: train_loss=0.3483 acc=0.8818 | val_loss=0.4784 acc=0.8264 | val_AUROC=0.8889  saved_best(AUROC=0.8889)  time=631.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 googlenet: train_loss=0.3256 acc=0.8949 | val_loss=0.5107 acc=0.8176 | val_AUROC=0.8846  time=629.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 googlenet: train_loss=0.3114 acc=0.9027 | val_loss=0.5192 acc=0.8114 | val_AUROC=0.8808  time=626.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 googlenet: train_loss=0.2765 acc=0.9208 | val_loss=0.5243 acc=0.8167 | val_AUROC=0.8831  time=629.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 googlenet: train_loss=0.2583 acc=0.9304 | val_loss=0.5397 acc=0.8170 | val_AUROC=0.8801  time=639.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 googlenet: train_loss=0.2481 acc=0.9364 | val_loss=0.5509 acc=0.8183 | val_AUROC=0.8815  time=625.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 googlenet: train_loss=0.2264 acc=0.9490 | val_loss=0.5599 acc=0.8136 | val_AUROC=0.8750  time=624.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "googlenet Epoch 18/50:  51%|█████▏    | 787/1534 [04:43<04:27,  2.79it/s, loss=0.2171, acc=0.9545]"
          ]
        }
      ]
    }
  ]
}